{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae51b85201ae23b9",
   "metadata": {},
   "source": [
    "# Using Distribution plot in MNIST\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/inspectus/blob/main/notebooks/mnist.ipynb)\n",
    "\n",
    "Here, we will use the MNIST dataset to train a simple Convolutional Neural Network (CNN) model. We will use the Distribution plot to visualize the training loss of the model.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install inspectus\n",
    "!pip install --upgrade altair\n",
    "!pip install labml"
   ],
   "id": "4a07f0ba910f371d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38629c23d71575a8",
   "metadata": {},
   "source": [
    "Model was taken from the PyTorch example: [github](https://github.com/pytorch/examples/tree/main/mnist)"
   ]
  },
  {
   "cell_type": "code",
   "id": "6608ec0cadfef67",
   "metadata": {},
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "601d977283876cd2",
   "metadata": {},
   "source": [
    "#### Data Logger\n",
    "\n",
    "A [data logger](data_logger.ipynb) is initialized for the path `mnist` and the data is cleared in case there are previous records."
   ]
  },
  {
   "cell_type": "code",
   "id": "fdc45bc9264c663f",
   "metadata": {},
   "source": [
    "import inspectus.data_logger\n",
    "\n",
    "step = 0\n",
    "d_log = inspectus.data_logger.DataLogger('mnist')\n",
    "d_log.clear()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6e33d447e2181db",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "At each training step, we find individual losses for every sample in the batch. This is done by setting the `reduction` parameter to `none` in the loss function. The loss is then saved to the data logger."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6281617c93e987e",
   "metadata": {},
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    global step\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < 2.0:\n",
    "            with torch.no_grad():\n",
    "                d_log.save('train_loss', F.nll_loss(output, target, reduction='none'), step)\n",
    "                step += 1\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "589955342b2360cc",
   "metadata": {},
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                          transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                          transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1, shuffle=False)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.05)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a00ab7257ac1d5ef",
   "metadata": {},
   "source": [
    "for epoch in range(2):\n",
    "    train(model, device, train_loader, optimizer, epoch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "965353eac5c27d4b",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd07563e5772c3ed",
   "metadata": {},
   "source": [
    "Here logged data are visualized using the Distribution plot. We also include the mean series in the plot and render the distribution with 5 bands\n",
    "\n",
    "![Training Loss](../assets/mnist_train_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ccc32b2b286671ed",
   "metadata": {},
   "source": [
    "tl = d_log.read('train_loss')\n",
    "inspectus.distribution({'train_loss': tl}, width=500, height=500)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5d9c6065ab5e0e9",
   "metadata": {},
   "source": [
    "The distribution is visualized using five bands, where the central band represents the median (50th percentile). The other bands correspond to specific percentiles: 0, 6.68, 15.87, 30.85, 50.00, 69.15, 84.13, 93.32, and 100.00.\n",
    "\n",
    "![Training Loss Zoomed in](../assets/mnist_train_loss_tail.png)\n",
    "\n",
    "From the plot, we observe that the median loss is relatively low, below 0.1. However, there are also significantly higher values like 9, which are outliers. These outliers increase the mean loss due to their larger magnitude of contribution, even though they represent a small portion of the data.\n",
    "\n",
    "Typically, we plot the mean loss at each training step. We also render the mean loss on the plot. Upon close inspection, we notice that the mean is higher than the median, aligning with the third band. This difference is due to the influence of outliers.\n",
    "\n",
    "In conclusion, the model's performance is actually better than what the mean loss indicates, excluding the outliers. To improve the model, we should investigate these outlier samples to determine if there are any issues with the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b81a933fbf8689",
   "metadata": {},
   "source": [
    "# Obtaining high loss samples\n",
    "\n",
    "To obtain the samples with the highest loss, we first load the model and the test dataset. We then calculate the loss for each sample in the dataset and select the top 10 samples with the highest loss. Finally, we visualize these samples to understand why the model failed to predict them correctly."
   ]
  },
  {
   "cell_type": "code",
   "id": "2b2adf3329bb6859",
   "metadata": {},
   "source": [
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "502678a1ef8740a9",
   "metadata": {},
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('../data', download=True, train=False,\n",
    "                          transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=100000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6df3e2ad5c4684c3",
   "metadata": {},
   "source": [
    "data, target = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "output = model(data.to(device))\n",
    "loss = F.nll_loss(output, target.to(device), reduction='none')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f523d1fabff9ba5e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_losses = torch.topk(loss, 10)\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 5))\n",
    "for i in range(10):\n",
    "    ax[i].imshow(data[top_losses.indices[i]].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    ax[i].set_title(f\"Ground truth: {target[top_losses.indices[i]].item()}\\nPrediction: {output[top_losses.indices[i]].argmax().item()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43866e42b63e895e",
   "metadata": {},
   "source": [
    "![High Loss Samples](../assets/mnist_outlier_samples.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "98b99bbde3cc32f3",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
